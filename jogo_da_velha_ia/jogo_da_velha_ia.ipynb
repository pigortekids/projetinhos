{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "name": "jogo_da_velha_ia.ipynb",
      "provenance": [],
      "collapsed_sections": []
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "accelerator": "GPU"
  },
  "cells": [
    {
      "cell_type": "code",
      "metadata": {
        "id": "Sx-QhGFyJtLV",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "import numpy as np\n",
        "import pandas as pd\n",
        "\n",
        "from tensorflow.keras import models\n",
        "from tensorflow.keras import layers\n",
        "from tensorflow.keras import optimizers\n",
        "\n",
        "from datetime import datetime\n",
        "import itertools\n",
        "import argparse\n",
        "import re\n",
        "import os\n",
        "import pickle\n",
        "\n",
        "from sklearn.preprocessing import StandardScaler\n",
        "\n",
        "import matplotlib.pyplot as plt\n",
        "import pandas as pd\n",
        "import time"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "1hmeRyvqL3Ao",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "### The experience replay memory ###\n",
        "class ReplayBuffer:\n",
        "  def __init__(self, obs_dim, act_dim, size):\n",
        "    self.obs1_buf = np.zeros([size, obs_dim], dtype=np.float32)\n",
        "    self.obs2_buf = np.zeros([size, obs_dim], dtype=np.float32)\n",
        "    self.acts_buf = np.zeros(size, dtype=np.uint8)\n",
        "    self.rews_buf = np.zeros(size, dtype=np.float32)\n",
        "    self.done_buf = np.zeros(size, dtype=np.uint8)\n",
        "    self.ptr, self.size, self.max_size = 0, 0, size\n",
        "\n",
        "  def store(self, obs, act, rew, next_obs, done):\n",
        "    self.obs1_buf[self.ptr] = obs\n",
        "    self.obs2_buf[self.ptr] = next_obs\n",
        "    self.acts_buf[self.ptr] = act\n",
        "    self.rews_buf[self.ptr] = rew\n",
        "    self.done_buf[self.ptr] = done\n",
        "    self.ptr = (self.ptr+1) % self.max_size\n",
        "    self.size = min(self.size+1, self.max_size)\n",
        "\n",
        "  def sample_batch(self, batch_size=32):\n",
        "    idxs = np.random.randint(0, self.size, size=batch_size)\n",
        "    return dict(s=self.obs1_buf[idxs],\n",
        "                s2=self.obs2_buf[idxs],\n",
        "                a=self.acts_buf[idxs],\n",
        "                r=self.rews_buf[idxs],\n",
        "                d=self.done_buf[idxs])"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "jFYQ76GZL5ym",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "def cria_pasta(pasta):\n",
        "  if not os.path.exists(pasta):\n",
        "    os.makedirs(pasta)"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "UD6coaeAUUoM",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "def cria_modelo(n_parametros, n_acoes):\n",
        "\n",
        "    rna = models.Sequential()\n",
        "    rna.add(layers.Dense(32, activation='relu', input_shape=(n_parametros,)))\n",
        "    rna.add(layers.Dense(16,activation='relu'))\n",
        "    rna.add(layers.Dense(n_acoes, activation='softmax'))\n",
        "\n",
        "    adam = optimizers.Adam(lr=0.001, beta_1=0.9, beta_2=0.999, decay=0.0)\n",
        "\n",
        "    rna.compile(optimizer=adam, loss='categorical_crossentropy', metrics=['accuracy'])\n",
        "\n",
        "    rna.summary()\n",
        "\n",
        "    return rna"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "rYcHiJpyNoQF",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "class JogoDaVelhaEnv:\n",
        "    def __init__(self):\n",
        "        self.campo = None\n",
        "        self.n_parametros = 9\n",
        "        self.n_acoes = 9\n",
        "        self.reseta(True)\n",
        "\n",
        "    def reseta(self, comeca):\n",
        "        self.campo = np.zeros(self.n_parametros)\n",
        "        if not comeca:\n",
        "            self.jogada_oponente()\n",
        "        return self.observacao()\n",
        "        \n",
        "    def observacao(self):\n",
        "        obs = np.empty(self.n_acoes)\n",
        "        for i in range(self.n_acoes):\n",
        "            obs[i] = self.campo[i]\n",
        "        return obs\n",
        "    \n",
        "    def acabou(self):\n",
        "\n",
        "        acabou = False\n",
        "        valor = 0\n",
        "        \n",
        "        for i in range(3):\n",
        "            if self.campo[0 + i] + self.campo[1 + i] + self.campo[2 + 1] in [3, -3]: #confere as linhas\n",
        "                valor = self.campo[0 + i] + self.campo[1 + i] + self.campo[2 + 1]\n",
        "            if self.campo[0 + i] + self.campo[3 + i] + self.campo[6 + i] in [3, -3]: #confere as colunas\n",
        "                valor = self.campo[0 + i] + self.campo[3 + i] + self.campo[6 + i]\n",
        "            \n",
        "        if self.campo[0] + self.campo[4] + self.campo[8] in [3, -3]:\n",
        "            valor = self.campo[0] + self.campo[4] + self.campo[8]\n",
        "        if self.campo[2] + self.campo[4] + self.campo[6] in [3, -3]:\n",
        "            valor = self.campo[2] + self.campo[4] + self.campo[6]\n",
        "\n",
        "        valor = int(valor / 3)\n",
        "        if valor != 0:\n",
        "            acabou = True\n",
        "        else:\n",
        "            acabou = True\n",
        "            for i in range(9):\n",
        "                if self.campo[i] == 0:\n",
        "                    acabou = False\n",
        "\n",
        "        return [acabou, valor]\n",
        "\n",
        "    def jogada_oponente(self):\n",
        "        valido = False\n",
        "\n",
        "        while not valido:\n",
        "            jogada = np.random.choice(self.n_acoes)\n",
        "            if self.campo[jogada] == 0:\n",
        "                self.campo[jogada] = -1\n",
        "                valido = True\n",
        "\n",
        "    def step(self, acao):\n",
        "\n",
        "        recompensa = None\n",
        "        valido = None\n",
        "\n",
        "        if self.campo[acao] == 0:\n",
        "            self.campo[acao] = 1\n",
        "            valido = True\n",
        "            acabou = self.acabou()\n",
        "            if acabou[0] and acabou[1] == 1:\n",
        "                recompensa = 1\n",
        "            elif acabou[0] and acabou[1] == 0:\n",
        "                recompensa = -0.5\n",
        "            else:\n",
        "                recompensa = 0.1\n",
        "        else:\n",
        "            recompensa = -0.1\n",
        "            valido = False\n",
        "\n",
        "        if valido and not self.acabou()[0]:\n",
        "            self.jogada_oponente()\n",
        "            acabou = self.acabou()\n",
        "            if acabou[0]:\n",
        "                recompensa = -1\n",
        "\n",
        "        # conform to the Gym API\n",
        "        return self.observacao(), recompensa, self.acabou()[0], self.acabou()[1]"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "aAtHn55uS1qU",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "class DQNAgente(object):\n",
        "    def __init__(self, n_parametros, n_acoes, n_episodios=1):\n",
        "        self.n_parametros = n_parametros\n",
        "        self.n_acoes = n_acoes\n",
        "        self.memoria = ReplayBuffer(n_parametros, n_acoes, size=100)\n",
        "        self.gamma = 0.95  # discount rate\n",
        "        self.epsilon = 1.0  # exploration rate\n",
        "        self.epsilon_min = 0.01\n",
        "        self.epsilon_decay = ( self.epsilon - self.epsilon_min ) / n_episodios\n",
        "        self.modelo = cria_modelo(n_parametros, n_acoes)\n",
        "\n",
        "    def acao(self, estado):\n",
        "        if np.random.rand() <= self.epsilon:\n",
        "            return np.random.choice(self.n_acoes)\n",
        "        vetor_acoes = self.modelo.predict( np.reshape( estado, (1, len(estado)) ) )\n",
        "        return np.argmax(vetor_acoes)  # returns action\n",
        "    \n",
        "    def atualiza_memoria(self, estado, acao, recompensa, proximo_estado, terminou):\n",
        "        self.memoria.store(estado, acao, recompensa, proximo_estado, terminou)\n",
        "\n",
        "    def treina(self, batch_size=1):\n",
        "        if self.memoria.size < batch_size: #confere se ja deve treinar\n",
        "            return\n",
        "\n",
        "        minibatch = self.memoria.sample_batch(batch_size) #pega a quantidade de linhas do batch_size para treinar\n",
        "        estados = minibatch['s'] #pega os estados\n",
        "        acoes = minibatch['a'] #pega as acoes\n",
        "        recompensas = minibatch['r'] #pega as recompensas\n",
        "        proximos_estados = minibatch['s2'] #pega os proximos estados\n",
        "        terminous = minibatch['d'] #pega os terminous\n",
        "\n",
        "        # Calculate the tentative target: Q(s',a)\n",
        "        objetivo = recompensas + (1 - terminous) * self.gamma * np.amax(self.modelo.predict(proximos_estados), axis=1)\n",
        "\n",
        "        objetivos = self.modelo.predict( np.reshape( estados, (len(estados), len(estados[0])) ) )\n",
        "        objetivos[np.arange(batch_size), acoes] = objetivo\n",
        "\n",
        "        # Run one training step\n",
        "        self.modelo.train_on_batch(estados, objetivos)\n",
        "\n",
        "    def decair_epsilon(self):\n",
        "        if self.epsilon > self.epsilon_min:\n",
        "            self.epsilon -= self.epsilon_decay\n",
        "\n",
        "    def salva_pesos(self, caminho):\n",
        "        self.modelo.save_weights(caminho)\n",
        "\n",
        "    def carrega_pesos(self, caminho):\n",
        "        self.modelo.load_weights(caminho)"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "zNli7FXFQcjp",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "def play_one_episode(agente, ambiente, treino, episodio):\n",
        "    comeca = (episodio % 2 == 0)\n",
        "    estado = ambiente.reseta(comeca)\n",
        "    terminou = False\n",
        "    ganhou = None\n",
        "    soma_recompensa = 0\n",
        "\n",
        "    while not terminou:\n",
        "        acao = agente.acao(estado)\n",
        "        proximo_estado, recompensa, terminou, ganhou = ambiente.step(acao)\n",
        "        if treino:\n",
        "            agente.atualiza_memoria(estado, acao, recompensa, proximo_estado, terminou)\n",
        "            agente.treina(batch_size)\n",
        "        estado = proximo_estado\n",
        "        soma_recompensa += recompensa\n",
        "\n",
        "    agente.decair_epsilon()\n",
        "    return ganhou, soma_recompensa"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "xZYRAnxZPeXQ",
        "colab_type": "code",
        "outputId": "32f748a2-6c31-4bbf-bcad-28d4d154f9ee",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 408
        }
      },
      "source": [
        "t_ini = datetime.now() #variavel para ver quanto tempo demora para rodar o codigo todo\n",
        "\n",
        "pasta_modelos = 'modelos_rl' #pasta para salvar os modelos\n",
        "pasta_recompensas = 'recompensas_rl' #pasta para salvar os resultados\n",
        "n_episodios = 3000 #numero de \"epocas\"\n",
        "divisor_val = 10 #variavel para definir a cada quantos episodios vai guardar os valores\n",
        "divisor_print = int(n_episodios / 10) #variavel para definir a cada quantos episodios vai mostrar os valores\n",
        "batch_size = 9 #treinar depois de batch_size jogadas tentadas\n",
        "\n",
        "args = {'modo':'treino'} #argumento para definir so modo que o algoritmo vai rodar (treino ou teste)\n",
        "\n",
        "cria_pasta(pasta_modelos) #cria a pasta de modelos caso ainda nao exista\n",
        "cria_pasta(pasta_recompensas) #cria a pasta de resultados caso ainda nao exista\n",
        "\n",
        "ambiente = JogoDaVelhaEnv() #cria o ambiente para o jogo\n",
        "n_parametros = ambiente.n_parametros #define o numero de entradas da rede\n",
        "n_acoes = ambiente.n_acoes #define o numero de saidas da rede\n",
        "agente = DQNAgente(n_parametros, n_acoes, n_episodios) #cria o agente que vai ser treinado\n",
        "\n",
        "ganhadores = [] #variavel para guardar quem ganhou em casa episodio\n",
        "recompensas = [] #variavel para guardar as recompensas de todos os episodios\n",
        "media_recompensas = [] #variavel para guardar a media das recompensas\n",
        "media_ia = [] #variavel para guardar as medias de vitoria da ia\n",
        "media_aleatorio = [] #variavel para guardar as medias de vitoria do oponente aleatorio\n",
        "media_velha = [] #variavel para guardar as medias de jogos que deram velha\n",
        "jogos = [] #variavel para guardar o resultado do jogo\n",
        "\n",
        "if args['modo'] == 'teste': #caso o algoritmo esteja em modo teste\n",
        "    agente.carrega_pesos(f'{pasta_modelos}/dqn.h5') #carrega os pesos da rede\n",
        "    agente.epsilon = 0 #define a aleatoriedade das acoes = 0%\n",
        "\n",
        "t_ini_ep = datetime.now() #variavel para ver quanto tempo demora para rodar os episodios\n",
        "for episodio in range(n_episodios): #roda para a quantidade de episodios especificada\n",
        "    resultado, recompensa = play_one_episode(agente, ambiente, args['modo'] == 'treino', episodio) #roda um episodio\n",
        "\n",
        "    ganhador = None #define uma variavel para quem ganhou o jogo\n",
        "    if resultado == 1: #se o resultado for 1\n",
        "        ganhador = 'IA' #o ganhador foi a IA\n",
        "    elif resultado == -1: #se o resultado for -1\n",
        "        ganhador = 'Aleatorio' #o ganhador foi o oponente aleatorio\n",
        "    else: #se o resultado for 0\n",
        "        ganhador = 'Deu velha' #o jogo deu velha\n",
        "\n",
        "    ganhadores.append(resultado) #adiciona quem ganhou na lista de ganhadores\n",
        "    recompensas.append(recompensa) #adiciona a recompensa final do episodio na lista de recompensas\n",
        "    jogos.append(ambiente.campo) #adiciona o resultado do jogo na variavel de jogos\n",
        "    \n",
        "    if (episodio + 1) % divisor_val == 0: #se a quantidade de episodios for divisivel pelo divisor de valores, guarda os dados\n",
        "        media_recompensas.append( sum(recompensas[-divisor_val:]) / divisor_val ) #adiciona a media das recompensas dos ultimos (divisor_val) episodios\n",
        "        media_ia.append( ganhadores[-divisor_val:].count(1) / divisor_val ) #adiciiona a media de vitorias da IA dos ultimos (divisor_val) episodios\n",
        "        media_aleatorio.append( ganhadores[-divisor_val:].count(-1) / divisor_val ) #adiciiona a media de vitorias do oponente aleatorio dos ultimos (divisor_val) episodios\n",
        "        media_velha.append( ganhadores[-divisor_val:].count(0) / divisor_val ) #adiciiona a media de jogos que deram velha dos ultimos (divisor_val) episodios\n",
        "\n",
        "    if (episodio + 1) % divisor_print == 0: #se a quantidade de episodios for divisivel pelo divisor de prints, printa os dados\n",
        "        if len(media_ia) > 0: #se ja tiverem valores\n",
        "            m_r = ( sum(recompensas[-divisor_print:]) / divisor_print ) * 100 #calcula a media de recompensas dos ultimos (divisor_print) episodios\n",
        "            m_ia = ( ganhadores[-divisor_print:].count(1) / divisor_print ) * 100 #calcula a media de vitorias da IA dos ultimos (divisor_print) episodios\n",
        "            m_ale = ( ganhadores[-divisor_print:].count(-1) / divisor_print ) * 100 #calcula a media de vitorias do oponente aleatorio dos ultimos (divisor_print) episodios\n",
        "            m_velha = ( ganhadores[-divisor_print:].count(0) / divisor_print ) * 100 #calcula a media de jogos que deram velha (divisor_print) episodios\n",
        "            t_fim_ep = datetime.now() - t_ini_ep #define quanto tempo demorou os episodios\n",
        "            print(f\"Episodio: {episodio + 1}/{n_episodios}, media recompensas: {m_r:.2f}, media IA: {m_ia:.2f}%, media Aleatorio: {m_ale:.2f}%, media Deu velha: {m_velha:.2f}%, tempo: {t_fim_ep}\") #printa as informacoes\n",
        "            t_ini_ep = datetime.now() #reseta a variavel de tempo dos episodios\n",
        "\n",
        "if args['modo'] == 'treino': #caso o algoritmo esteja em modo treino\n",
        "    agente.salva_pesos(f'{pasta_modelos}/dqn.h5') #salva os pesos da rede\n",
        "\n",
        "np.save(f'{pasta_recompensas}/{args[\"modo\"]}.npy', [ganhadores, jogos]) #salva os resultados de todos os episodios\n",
        "\n",
        "t_fim = datetime.now() - t_ini #define quanto tempo demorou o algoritmo todo\n",
        "print(args['modo'] + 'finalizado em ' + str(t_fim)) #mostra quanto tempo demorou"
      ],
      "execution_count": 0,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Model: \"sequential\"\n",
            "_________________________________________________________________\n",
            "Layer (type)                 Output Shape              Param #   \n",
            "=================================================================\n",
            "dense (Dense)                (None, 32)                320       \n",
            "_________________________________________________________________\n",
            "dense_1 (Dense)              (None, 16)                528       \n",
            "_________________________________________________________________\n",
            "dense_2 (Dense)              (None, 9)                 153       \n",
            "=================================================================\n",
            "Total params: 1,001\n",
            "Trainable params: 1,001\n",
            "Non-trainable params: 0\n",
            "_________________________________________________________________\n",
            "Episodio: 300/3000, media recompensas: -33.40, media IA: 40.00%, media Aleatorio: 42.67%, media Deu velha: 17.33%, tempo: 0:02:00.677066\n",
            "Episodio: 600/3000, media recompensas: -24.43, media IA: 44.67%, media Aleatorio: 39.67%, media Deu velha: 15.67%, tempo: 0:02:05.041664\n",
            "Episodio: 900/3000, media recompensas: -31.40, media IA: 45.67%, media Aleatorio: 35.33%, media Deu velha: 19.00%, tempo: 0:02:21.524401\n",
            "Episodio: 1200/3000, media recompensas: -29.10, media IA: 47.33%, media Aleatorio: 36.67%, media Deu velha: 16.00%, tempo: 0:02:28.689749\n",
            "Episodio: 1500/3000, media recompensas: -21.93, media IA: 55.00%, media Aleatorio: 26.67%, media Deu velha: 18.33%, tempo: 0:02:44.128293\n",
            "Episodio: 1800/3000, media recompensas: 1.50, media IA: 71.33%, media Aleatorio: 19.00%, media Deu velha: 9.67%, tempo: 0:02:49.350830\n",
            "Episodio: 2100/3000, media recompensas: -7.90, media IA: 73.00%, media Aleatorio: 15.33%, media Deu velha: 11.67%, tempo: 0:03:21.354215\n",
            "Episodio: 2400/3000, media recompensas: -2.63, media IA: 74.33%, media Aleatorio: 17.00%, media Deu velha: 8.67%, tempo: 0:03:09.006863\n",
            "Episodio: 2700/3000, media recompensas: -58.70, media IA: 82.00%, media Aleatorio: 12.33%, media Deu velha: 5.67%, tempo: 0:05:30.036377\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "vCBN-azqY9T7",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "x_plot = np.arange(len(media_recompensas)) #define variavel para plotar em x no primeiro grafico\n",
        "\n",
        "fig, axs = plt.subplots(2, sharex=True)\n",
        "fig.suptitle(f'Medias a cada {divisor_val} episodios')\n",
        "axs[0].set(ylabel='vitorias')\n",
        "axs[0].plot(x_plot, media_ia, label='IA')\n",
        "axs[0].plot(x_plot, media_aleatorio, label='Aleatorio')\n",
        "axs[0].plot(x_plot, media_velha, label='Deu velha')\n",
        "axs[0].legend() #adiciona uma legenda\n",
        "axs[1].set(ylabel='recompensa')\n",
        "axs[1].plot(x_plot, media_recompensas)\n",
        "\n",
        "fig.savefig('result.png', bbox_inches='tight') #salva o grafico em uma foto"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Ewum4j3u5SxT",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "#jogar contra a maquina\n",
        "\n",
        "ambiente = JogoDaVelhaEnv() #cria o ambiente para o jogo\n",
        "agente = DQNAgente(n_parametros, n_acoes, n_episodios)\n",
        "agente.carrega_pesos(f'{pasta_modelos}/dqn.h5') #carrega os pesos da rede\n",
        "agente.epsilon = 0 #define a aleatoriedade das acoes = 0%\n",
        "\n",
        "comecar = True\n",
        "if not comecar:\n",
        "    ambiente.campo[ agente.acao( ambiente.observacao() ) ] = -1\n",
        "\n",
        "while not ambiente.acabou()[0]:\n",
        "    #jogada do jogador\n",
        "    jogada_valida = False\n",
        "    while not jogada_valida:\n",
        "        jogada = int(input('digite o campo a jogar: '))\n",
        "        if campo[ jogada ] != 0:\n",
        "            print('campo inv√°lido')\n",
        "        else:\n",
        "            jogada_valida = True\n",
        "\n",
        "    #jogada da maquina\n",
        "    jogada_valida = False\n",
        "    while not jogada_valida:\n",
        "        jogada = agente.acao( ambiente.observacao() )\n",
        "        if campo[ jogada ] == 0:\n",
        "            ambiente.campo[ jogada ] = -1\n",
        "            jogada_valida = True\n",
        "\n",
        "resultado = ambiente.acabou()[1]\n",
        "ganhador = None #define uma variavel para quem ganhou o jogo\n",
        "if resultado == 1: #se o resultado for 1\n",
        "    ganhador = 'Voce ganhou' #o ganhador foi a IA\n",
        "elif resultado == -1: #se o resultado for -1\n",
        "    ganhador = 'A IA ganhou' #o ganhador foi o oponente aleatorio\n",
        "else: #se o resultado for 0\n",
        "    ganhador = 'Deu velha' #o jogo deu velha\n",
        "\n",
        "print(ganhador)"
      ],
      "execution_count": 0,
      "outputs": []
    }
  ]
}